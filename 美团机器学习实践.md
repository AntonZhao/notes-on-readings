# 美团机器学习实践
> [官网](http://www.ituring.com.cn/book/2573)
> [douban](https://book.douban.com/subject/30243136/)

## 相关

[如何理解Nvidia英伟达的Multi-GPU多卡通信框架NCCL？](https://www.zhihu.com/question/63219175)

[美团深度学习系统的工程实践](https://tech.meituan.com/dl_system_in_nlu_and_speech.html)

**并行方案**：

 - 数据并行：每个计算单元保留一份完整的模型拷贝，分别训练不同的数据。
	 - 是最常见的训练方式。
 - 模型并行：各个计算单元存储**同一层模型的不同部分**，训练相同的数据。
	 - 频繁的同步通信导致系统不能充分地利用硬件的运算能力，所以更为少见。
	 - 一些业务场景下，Softmax层需要分类的类别可能会有很多，导致Softmax层太大，单个计算单元无法存储，这个时候，需要把模型切割成若干部分，存储在不同的运算单元。
 - 流式并行：每个计算单元都存储**不同层的模型**，训练相同的数据。
	 - 好处：每个运算单元之间的通信和计算重叠（overlap），配置得当可以非常充分地利用硬件资源。
	 - 缺点：根据不同的模型，需要平衡好各个计算单元的计算量，配置不好很容易形成“堰塞湖”。
 - 混合并行：各种并行方式的混合。
	 - 如对于一些图像识别任务来说，可能前几层使用数据并行，最后的Softmax层，使用模型并行。

**通信方案**：
> 以Nvidia NCCL单机多卡的通信解决方案为例。多机多卡的通信解决方案类似。

常见的Collective communication类型：
![](https://tech.meituan.com/img/dl_system_in_nlu_and_speech/1535459386136.png)
对于深度学习训练而言，关键的两种通信类型为Broadcast和Reduce：

 - Broadcast用于Master分发最新的模型给各个GPU。
 - Reduce 用于各个GPU计算完Mini-batch后，把模型更新值汇总到Master上。

一种简单的优化方法是把所需要传输的数据分成若干块，然后通过接力的方式逐个传递，每个GPU都把自己最新的一块数据发送到下一个GPU卡上。
![](https://tech.meituan.com/img/dl_system_in_nlu_and_speech/1535459756386.png)
使满足数据的份数大于节点数，则通信时间不随节点数的增加而增加，只和数据总量以及带宽有关。

# 通用流程

## 问题建模

## 特征工程

## 常用模型

## 模型融合

# 数据挖掘

# 搜索和推荐

# 计算广告

# 深度学习

# 算法工程

## 大规模机器学习
> [笔记](https://coladrill.github.io/2018/10/06/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/)

### 并行计算编程技术

 - CPU单核：通过向量化技术来提升单核的处理能力；
 - CPU多核：通过多线程技术来充分利用多核处理能力；
 - GPU：通过异构计算来扩充单机的处理能力；
 - 多机并行：把多机串联起来组成计算集群；

#### 向量化

把指令应用于一个数组/向量。

在X86体系架构的CPU上，常用的向量化编程技术是SSE和AVX。

 - SSE指令集（Streaming SIMD Extensions, 单指令多数据流式扩展）：16个128位的寄存器，每一个寄存器可存放4个（32位）单精度浮点数。这些数可以在寄存器中进行算术逻辑运算，然后把结果放回内存。理论加速比为4倍
 - AVX指令集（Advanced Vector Extensions，高级向量扩展）：将16个128位寄存器扩充为256位，，从而支持256位的矢量计算。理论加速比为8倍

#### 多核并行OpenMP

 - 基于共享存储体系结构；
 - 通过在串行程序中添加OpenMP指令和调用OpenMP库函数来实现在共享内存系统上的并行执行。很方便对传统程序进行并行化改造；
 - 简单灵活的接口模型，移植性好

Session指令

#### GPU编程

GPU相比CPU优势：

 - CPU主要为串行指令优化，而GPU主要为并行计算优化；
 - 同样芯片面积下更多的计算单元；
 - 更高的带宽。

CUDA相关概念：

 - 线程、线程块、线程网络：一个网格分为多个线程块，一个线程块分为多个线程
 - 线程束：GPU执行程序时的调度单位
 - 流处理器：最基本的处理单元
 - 流多处理器

#### 多机并行MPI

消息传递接口（Message Passing Interface，MPI）是消息传递函数库的标准规范。

MPI有上百个函数调用接口，Fortran和C/C++可以直接对这些函数进行调用。常用的MPI函数有限，主要有：

 - MPI_Init()：初始化
 - MPI_Comm_size()：获取进程数
 - MPI_Comm_rank()：获取进程序号
 - MPI_Send()：发送消息
 - MPI_Recv()：接收消息
 - MPI_Finalize()：并行结束函数

MPI主要解决了进程之间的通信问题，包括跨主机的进程通信。基本功能：

 - 发送消息进行数据移动；
 - 接收消息进行数据规约。

![](http://images2.imagebam.com/05/17/04/22824d1076265144.png)

#### 小结

![](https://coladrill.github.io/img/post/20181006/4.png)

### 并行计算模型

分布式机器学习任务与传统分布式任务见的区别：

 - 机器学习任务中间结果具有容错性；
 - 模型中的参数往往非独立，具有更复杂的结构依赖；
 - 参数收敛速度不均匀。

分布式机器学习主要解决求解的并行化问题，主要是梯度下降求解的并行化问题。为此提出了很多并行模型。

#### BSP

在BSP模型中，计算由一系列用**全局同步**分开的周期为L的计算组成，这些计算成为超级步。

优点：计算和通信任务分开，简化通信协议；提供了执行紧耦合同步式并行算法的有效方式。
缺点：木桶效应。

#### SSP

最快的Worker比最慢的Worker超过设置的Bound时，所有的Worker进行一次参数同步。Bound可以根据迭代的次数，或者更新的参数差值来确定。

SSP可以有效平衡计算和网络通信的开销。

#### ASP

异步更新，Worker间无需同步。

#### 参数服务器

计算节点负责对分配到的自己本地的训练数据进行计算，并更新对应的参数。
参数服务节点采用分布式存储的方式，各自存储全局参数的一部分，并接受计算节点的查询和数据更新请求。

梯度更新方法：

 - 参数平均法：可以证明在数学上等同于单机训练。
 - 基于异步梯度下降的基于更新方法：只传递梯度更新信息。
梯度更新时机：
	 - Sequential：同步
	 - Eventual：异步
	 - Bounded Delay：折中方案

![](http://images2.imagebam.com/13/8b/5d/8178911076287454.png)

### 并行计算案例

#### XGBoost并行库Rabit

#### MXNet并行库PS-Lite

PS-Lite是MXNet分布式实现的核心，基于PS模型。特点：

 - 高效的通信：异步通信不会拖慢计算
 - 弹性一致：通过设置bound
 - 错误容忍：向量时钟容许网络错误；需要更新的参数都是Range-based，range内参数共享同一个时间戳。
 - 扩展性强：增加节点无需重启集群。采用一致性哈希
 - 易用性

PS-Lite的核心是KVStore，用于进行数据交换，提供数种数据一致性模型。主要函数 push 和 pull。

## 特征工程和实验平台

### 特征平台

#### 特征生产

离线特征

实时特征

#### 特征上线

**键值特征**加载框架

**服务特征**加载框架
服务特征通过RPC调用从其他远程服务获取，特征之间存在时序上的依赖。

DAG加载模型

**复合特征**加载框架
灵活配置特征组合

#### 在线特征监控

### 实验管理平台

A/B测试

