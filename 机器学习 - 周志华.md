# 机器学习
> [douban](https://book.douban.com/subject/26708119/)
> [勘误](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm)
> [书PDF](http://www.chinacloud.cn/upload/2017-02/201702141641.pdf)，[课件PPT](http://download.csdn.net/download/pingguolou/9830772)

## 第1章 绪论

### 术语

样本和**特征**
 - 数据集（data set）
 - 样本（sample）、示例（instance）、特征向量（feature vector）
 - 特征（feature）、属性（attribute）
 - 属性值（attribute value）
 - 样本空间（sample space）、属性空间（attribute space）、输入空间

从数据中学得模型的过程称为“学习”或“**训练**”.
 - 训练（training）、学习（learning）
 - 模型、学习器（learner）
 - 假设（hypothesis）
 - 真实（ground-truth）

**预测**任务是通过对训练集进行学习，建立一个从输入空间到输出空间的映射.
 - 预测（prediction）
 - **标记**（label）
 - 样例（example）：拥有标记信息的样本
 - 标记空间（label space）、输出空间

使用模型进行预测的过程称为**测试**（testing）.
 - 测试样本

**泛化**（generalization）：模型适用于新样本的能力
 - 分布（distribution）
 - 独立同分布（independent and identically distributed，简称 $i.i.d.$）

### 假设空间

归纳（induction）学习：特殊到一般的泛化过程

 - 广义：从样例中学习
	 - 监督学习（supervised learning）
		 - 回归（regression）：预测的是连续值
		 - 分类（classification）：预测的是离散值
	 - 无监督学习（unsupervised learning）
		 - 聚类
 - 狭义：概念学习. 从训练数据中学得概念

可以把学习过程看作一个**在假设空间（所有假设组成的空间）进行搜索**的过程，搜索的目标是找到与训练集“匹配”的假设。结果可能有多个假设（一个“假设集合”）与训练集一致，称之为“**版本空间**”（version space）.

### 归纳偏好

通过学习得到的模型对应了假设空间中的一个假设。算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias）.

原则：
 - 奥卡姆剃刀：选最简单
 - NFL(No Free Lunch Theorem)定理：比较算法优劣必须要针对具体学习问题

### 发展历程
 1. 推理期（Logic Reasoning）：1956 ~ 1960s，自动定理证明系统
 2. 知识期（Knowledge Engineering）：1970s ~ 1980s，专家系统
 3. 机器学习（Machine Learning）：1990s ~ now
	 - 基于神经网络的“**连接**主义”：BP算法
	 - 基于**逻辑**表示的“符号主义”：决策树学习、基于逻辑的学习
	 - **统计**学习：SVM、核方法（kernel methods）
	 - 深度学习：多层神经网络

### 应用

数据挖掘

 - 数据管理：数据库
 - 数据分析：机器学习

### 阅读材料

书籍

 - 

会议

 - 机器学习：ICML(以及ECML, ACML), NIPS, COLT；国内CCML, MLA
 - 人工智能：IJCAI, AAAI
 - 数据挖掘：KDD, ICDM
 - 计算机视觉与模式识别：CVPR

期刊

 - 机器学习：Journal of Machine Learning Research, Machine Learning
 - 人工智能：Artificial Intelligence, Journal of Artificial Intelligence Research
 - 数据挖掘：ACM Transactions on Knowledge Discovery from Data, Data Mining and Knowledge Discovery
 - 计算机视觉与模式识别：IEEE Transactions on Pattern Analysis and Machine Intelligence
 - 神经网络：Neural Computation, IEEE Transactions on Neural Networks and Learning Systems
 - 统计学：Annals of Statistics

## 第2章 模型评估与选择

模型选择的三个关键问题

 - 评估方法 -> 获得测试结果
 - 性能度量 -> 评估性能优劣
 - 比较检验 -> 判断实质差别

### 经验误差与过拟合

**术语**
 - 精度（accuracy）
 - 训练误差（training error）、经验误差（empirical error）
 - 泛化误差（generalization error）
 - 过拟合（overfitting）
 - 欠拟合（underfitting）

我们希望得到泛化误差小的模型.

过拟合是机器学习面临的关键障碍。无法彻底避免，只能缓解.

### 评估方法
以测试误差作为泛化误差的近似.
训练/测试集的划分要尽可能保持数据分布的一致性.
分层采样（stratified sampling）：保留类别比例的采样.

对包含$m$个样本的数据集$D$，训练集$S$，测试集$T$：
 - **留出法**（hold-out）
将$D$划分为互斥两个集合，即$D = S \cup T, S \cap T = \varnothing$. 通常将2/3 ~ 4/5的样本用于训练，剩余用于测试. 
一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果. 

 - **k折交叉检验法**（k-fold cross validation）
将$D$划分成$k$个大小相似的互斥子集，每个子集$D_{i}$从$D$中通过分层采样得到。每次用$k-1$个子集作为训练集，剩下那个子集作为测试集，进行$k$次训练和测试，最终返回$k$次结果的均值. $k$最常取值为10.
为减小样本划分引入的差异，通常要随机使用不同划分重复$p$次，最终取$p$次$k$折交叉验证结果的均值.
	 - 留一法（Leave-One-Out，简称LOO）：令$k=m$

 - **自助法**（bootstrapping），也叫可重复采样、有放回采样
对$D$采样得$D'$，$m$次随机放一个样本到$D'$并取回，最终得到**包含$m$个样本**的$D'$. 最终$D$中约有36.8%的样本未出现在$D'$中，可将$D'$用作训练集，$D - D'$作为测试集. 
自助法在数据集小、难以划分训练/测试集时很有用；但由于改变了初始数据集的分布，会引入估计偏差.
	 - [包外估计](https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf)（out-of-bag estimate）：指用于测试的样本没在训练集中出现的测试结果.

调参与最终模型：
现实中常用做法是对每个参数选定范围和变化步长进行训练评估.
把训练数据另外划分为训练集和验证集（validation set），基于验证集上的性能进行模型选择和调参.
模型训练完成后，学习算法和参数配置已选定，此时应该用数据集$D$重新训练模型.

### 性能度量

性能度量反映了任务需求.

回归任务最常用的性能度量是“均方误差”.

#### 分类任务常用性能度量：
 - 错误率和精度
 - 查准率、查全率与F1：均等代价
查准率（precision）、准确率：以预测为基准
查全率（recall）、召回率：以真实为基准
分类[混淆矩阵](https://en.wikipedia.org/wiki/Confusion_matrix)（confusion matrix）
P-R曲线：P为纵轴R为横轴. 比较方法：
	 - 若一条曲线完全包住另一条，则前者性能优于后者. 若有交叉，面积大的较好
     - 平衡点（Break-Even Point，简称BEP）：“查准率=查全率”的取值，即$P=R$.
     - 为反映对查准率/查全率的不同偏好：
    $$
    F_{\beta} = \frac{(1 + \beta^{2}) \times P \times R}{(\beta^{2} \times P) + R}
    $$
    其中$\beta > 0$度量了**查全率对查准率**的相对重要性. 当$\beta = 1$时，退化为标准的F1.
	 - 宏查准率；宏查全率；宏F1：算完求平均
	 - 微查准率；微查全率；微F1：求平均后再算
 - ROC与AUC：比较方法与P-R曲线类似
ROC曲线以“真正例率”TPR为纵轴，“假正例率”FPR为横轴. AUC为ROC曲线下的面积.
排序损失 = 1 - AUC
ROC曲线绘制
 - **非均等代价**（unequal cost）：在非均等代价下，目标是最小化总体代价，而不再是最小化错误次数. （机器学习过程涉及许多类型的代价，如分类代价、测试代价、标记代价、属性代价等）
代价矩阵（cost matrix）
代价曲线（cost curve）：横轴是正例概率代价，纵轴是归一化代价.
代价曲线的绘制：将ROC曲线每个点转化为代价平面上一条线段，所有线段下界围成的面积.

### 比较检验
> 见PPT

**假设检验**（hypothesis test）

 - 二项检验（binomial test）：（一次留出法）
 - $t$ 检验（t-test）：（多次重复留出法或交叉验证法）
 - 交叉验证 $t$ 检验：在一个数据集上比较两个算法性能
 - McNemar检验：在一个数据集上比较两个算法性能
 - Friedman检验与Nemenyi后续检验：在一组数据集上对多个算法进行比较

### 附：
**参数估计**
点估计

 - 最大似然估计法

区间估计
 - 置信度（confidence）：$1 - \alpha$
 - 显著度：$\alpha$
 - 单边/双边假设

**抽样分布**

 - 二项分布：$X \sim B(n, p)$.
 - 标准正态分布：$X \sim N(0, 1)$. 概率密度 $f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$.
 - $\chi^{2}$ 分布：设 $X \sim N(0, 1)$，$X_i$ 相互独立，称 $\sum_{i=1}^n X_i^2$ 服从自由度为 $n$ 的卡方分布，记作 $\chi^2 \sim \chi^2(n)$
 - $t$ 分布：设 $X \sim N(0, 1)$，$Y \sim \chi^2(n)$，$X$ 与 $Y$ 相互独立，称 $t = \frac{X}{\sqrt{Y/n}}$ 服从自由度为 $n$ 的 $t$ 分布，记作 $t \sim t(n)$.
 - $F$ 分布：设 $X \sim \chi^2(n_1)$，$Y \sim \chi^2(n_2)$，$X$ 与 $Y$ 独立，称 $F = \frac{X/n_1}{Y/n_2}$ 服从自由度为 $(n_1, n_2)$ 的 $F$ 分布，记作 $F \sim F(n_1, n_2)$.

### 偏差与方差

泛化误差可分解为偏差、方差和噪声之和：$E(f;D) = bias^{2}(x) + var(x) + \varepsilon^{2}$
说明了泛化性能是由**学习算法的能力，数据的充分性和学习任务本身的难度**共同决定的.

 - **偏差**度量了学习算法本身的拟合能力
 - **方差**度量了数据扰动所造成的影响
 - **噪声**刻画了学习问题本身的难度

偏差-方差窘境（bias-variance dilemma）：随着训练进行，偏差越来越小，而方差越来越大. 导致泛化误差先减小后增大，造成过拟合.

## 第3章 线性模型

很多非线性模型可在线性模型基础上，通过引入层级结构或高维映射而得. 线性模型有很好的可解释性.

### 线性回归

多元线性回归（multivariate linear regression）试图学得$f(\bm{x_i}) = \bm{w}^T\bm{x_i} + b$，使得 $f(\bm{x_i})\simeq{y_i}$. 
对离散属性：

 - 存在“序”（order）关系，通过连续化转为连续值
 - 不存在序关系，转化为 $k$ 维向量

最小二乘法（least square method）：基于**均方误差最小化**来进行模型求解.

广义线性模型（generalized linear model）：用线性模型逼近 $y$ 的衍生物. 
$$
y = g^{-1}(\bm{w}^{T}\bm{x} + b)
$$
其中单调可微函数 $g(\cdot)$ 称为联系函数（link function）. 
当 $g(\cdot)=\ln(\cdot)$ 时，有：$\ln{y} = \bm{w}^T\bm{x} + b$，称为**对数线性回归**（log-linear regression）.

### 对数几率回归

二分类任务的输出 $y \in \{0, 1\}$，常用对数几率函数（logistic function）$y = \frac{1}{1 + e^{-(\bm{w}^T\bm{x}+b)}}$ 将线性模型输出的实值转换为近 $0/1$ 值. 得：
$$
y = \frac{1}{1 + e^{-(\bm{w}^T\bm{x}+b)}}
$$
称为**对数几率回归**（logistic regression）. 形式可变化为 $ln\frac{y}{1-y} = \bm{w}^T\bm{x} + b$，称 $\frac{y}{1-y}$ 为几率（odds），反映了 $\bm{x}$ 作为正例的相对可能性.

对数几率**优点**：

 - 无需事先假设数据分布
 - 可得到“类别”的近似概率分布（对数几率回归是**分类**学习方法！）
 - 可直接应用现有数值优化算法取得最优解

可通过“极大似然法”（maximum likelihood method）来估计 $\bm{w}$ 和 $b$.

### 线性判别分析

线性判别分析（Linear Discriminant Analysis，简称LDA）思想：将样例投影到一条直线（低维空间），使得同类样例的投影点尽可能接近，异类尽可能远离。是一种“监督降维”技术.

多分类LDA

### 多分类学习

多分类学习的基本思路是“拆解法”：将多分类任务拆分为若干个二分类任务求解.

拆分策略

 - 一对一（OvO）：将 $N$ 个类别两两配对，产生 $N(N-1)/2$ 个二分类任务，得到的 $N(N-1)/2$
个分类结果中，预测最多的类别作为最终分类结果
	 - 存储开销和测试时间大；只用两个类的样例，训练时间短
 - 一对余（OvR）：每次将一个类的样例作为正例、所有其他类的样例作为反例，训练 $N$ 个分类器，选择置信度最大的类别标记作为结果
	 - 存储开销和测试时间小；训练用到全部训练样例，训练时间长
 - 多对多（MvM）：每次将若干个类别作为正类、若干个其他类别作为反类
	 - 纠错输出码（Error Correcting Output Codes，简称ECOC）：引入编码思想
		 - 编码：对 $N$ 个类别作 $M$ 次划分，得到 $M$ 个二类任务，原类对应长为 $M$ 的编码
		 - 解码：测试样本交给 $M$ 个分类器预测，得到长为 $M$ 的预测结果编码，再与原类编码进行比较，距离最小的类别作为结果
		 - 特性：对分类器错误有一定容忍和修正能力，编码越长纠错能力最强

集成策略

 - 

### 类别不平衡问题

指不同类别的训练样例数目相差很大.

思路：几率 $\frac{y}{1-y} > \frac{m^+}{m^-}$ 则预测为正例.

策略：再缩放（rescaling），令 $\frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^-}{m^+}$

由于“无偏采样”现实中往往并不成立，常用类别不平衡学习方法有三类：

 - 欠采样（undersampling）：EasyEnsemble
 - 过采样（oversampling）：SMOTE
 - 阈值移动（threshold-moving）

 ### 拓展
[多标记学习](http://romisatriawahono.net/lecture/rm/survey/machine%20learning/Zhang%20-%20Multi-Label%20Learning%20Algorithms%20-%202013.pdf)

## 第4章 决策树

## 第5章 神经网络

定义：由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的**交互**反应.

M-P **神经元模型**：
$$
y = f(\sum_{i=1}^{n}{w_i}{x_i} - \theta)
$$
其中，$y$ 为输出，$\theta$ 称为阈值（即负偏差？），$f$ 称为激活函数.

### 多层前馈神经网络

感知机（Perceptron）：两层神经元组成，只有一层功能神经元.

**多层前馈神经网络**：每层神经元与下一层神经元全互联，神经元之间不存在同层连接或跨层连接（“前馈”指网络拓扑结构上不存在环或回路，并不意味着信号不能向后传）.

神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值.

多层前馈神经网络具有强大的表示能力，只需一个包含足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数.
如何设置神经元个数实际中常用试错法（trial-by-error）调整.

### BP算法

误差逆传播（error BackPropagation）算法基于梯度下降，以目标的负梯度方向对参数进行调整. 实质是 LMS（Least Mean Square）算法的推广，亦称广义 $\delta$ 规则.

**标准BP算法**：每次更新仅针对单个样例，参数更新频繁，不同样例间更新效果可能“抵销”，因此往往需要更多次数的迭代.

推导：TODO

过程：TODO

**累积BP算法**：基于累积误差最小化. 进行一轮学习（读取训练集一遍）后才对参数进行更新，更新频率低.

很多任务中，累积误差下降到一定程度后，进一步下降会非常缓慢，这时标准BP往往会获得较好的解，尤其当训练集非常大时.

缓解BP网络过拟合策略：

 - 早停（early stopping）
	 - 训练误差连续 a 轮的变化小于 b，则停止训练.
	 - 训练集误差降低但验证集误差升高，则停止训练.
 - 正则化：误差$E = \lambda\frac{1}{m}\sum_{k=1}^{m}E_k + (1-\lambda)\sum_{i}w_i^2$，$\lambda$ 常通过交叉验证法来估计.

### 全局最小与局部极小

神经网络的训练过程可看作在参数空间中寻找一组最优参数使得 $E$ 最小. 
基于梯度的搜索是使用最为广泛的参数寻优方法，每次迭代中，先计算误差函数在当前点的梯度，然后沿着负梯度方向搜索最优解.

跳出“局部极小”的策略（大为启发式，缺乏理论保障）：

 - 从不同的初始点开始搜索
 - 模拟退火（simulated annealing）：每一步都以一定概率接受比当前解更差的结果
 - 随机梯度下降：在计算梯度时加入随机因素
 - 遗传算法

### 其他常见神经网络
> 了解特点及训练方法

 - RBF（Radial Basis Function，径向基函数）网络：单隐层. **分类**任务中除BP外最常用
 - ART（Adaptive Resonance Theory，自适应谐振理论）网络：由比较层、识别层、识别阈值和重置模块构成. 胜者通吃（winner-take-all），是竞争学习（competitive learning）的代表.
	 - 较好的缓解了竞争学习中的“可塑性-稳定性窘境”（stability-plasticity dilemma），可进行增量学习（incremental learning）或在线学习（online learning）.
 - SOM（Self-Organization Map，自组织映射）网络：能将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元. 是最常用的**聚类**方法之一
 - 级联相关（Cascade-Correlation）网络：希望在训练过程中找到最符合数据特点的网络结构. 是“构造性”神经网络的代表
 - Elman网络：允许网络中出现环形结构，从而让一些神经元的输出反馈回来作为输入信号. RNN（recurrent neural network，递归神经网络）的代表
 - Boltzmann机：基于能量的模型. 显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达. 
	 - 受限Boltzmann机（RBM）：仅保留显层与隐层之间的连接（简化为二部图）. 常用对比散度（Contrastive Divergence）算法进行训练

### 深度学习

背景：计算能力的大幅度提高可缓解训练低效性，训练数据的大幅度增加可降低过拟合风险.

增加深度（隐层数）比增加宽度（隐层神经元数）更有效.

多隐层神经网络的**训练**：多隐层堆叠，**每层对上一层输出进行处理，逐渐将初始的低层特征表示转化为高层特征表示**

 - 无监督逐层训练：**预训练（pre-traning） + 微调（fine-tuning）**. 如DBN(deep belief network)
	 - 预训练：每次训练一层隐结点，将上一层隐结点输出作为输入，本层输出作为下一层输入
	 - 微调：预训练全部完成后，对整个网络进行“微调”训练
 - **权共享**（weight sharing）：让一组神经元使用相同的连接权. 如CNN
 - Drop out：每轮训练时随机选择一些隐结点令其权重不被更新（下轮可更新）
 - ReLU：激活函数改为修正线性函数，缓解梯度消失现象.

**CNN**（Convolutional Neural Network，卷积神经网络）：复合多个“卷积层”和“采样层”对输入信号进行加工，然后在连接层实现与输出目标之间的映射.

 - 卷积层：包含多个特征映射，每个特征映射是由多个神经元构成的“平面”，通过一种卷积滤波器提取输入的一种特征.
 - 采样层（汇合层）：基于局部相关性原理进行亚采样，在减少数据量的同时保留有用信息.
 - 连接层：就是传统神经网络对隐层与输出层的全连接.

深度学习最重要的特征：表示学习（特征学习）和联合优化.
**特征学习**：通过机器学习技术自身来产生好特征.

## 第6章 支持向量机

## 第7章 贝叶斯分类器

## 第8章 集成学习

## 第9章 聚类

## 第10章 降维与度量学习

## 第11章 特征选择与稀疏学习

## 第12章 计算学习理论

## 第13章 半监督学习

## 第14章 概率图模型

## 第15章 规则学习

## 第16章 强化学习
