# 分布式机器学习：算法、理论与实践
> [douban](https://book.douban.com/subject/30360968/)
> [思维导图](https://zhuanlan.zhihu.com/p/47999987)

## 绪论

### 大规模、分布式机器学习

分布式机器学习系统的独特性：

 - 对数据细节的微小差别具有很强的鲁棒性。只要求学习到的模型具有**统计一致性**。
 - 终极目标是追求模型的测试精度和泛化能力。不能一味追求计算快。

## 机器学习基础

### 基本概念

### 基本流程
> 以有监督二分类为例

误差

 - 示性误差函数
 - 经验误差风险：分类器模型在所有的训练集样本上所犯错误的总和。

损失

 - 损失函数：误差函数的优化，有很好的数学性质。
 - 经验损失风险

**附**：
[似然函数](https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)：一种关于统计模型中的参数的函数。
**概率**用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而**似然性**则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

### 常用的损失函数

#### Hinge

#### 指数

#### 交叉熵

### 常用的机器学习模型

#### 线性模型

通常使用对数几率函数对线性模型的输出进行变换：$g(x;w)=\frac{1}{1+exp(-w^Tx)}$，变换后的对数几率模型非常适合用对数似然损失或交叉熵损失进行训练。

广义线性函数：由一个线性函数派生出来的非线性函数。

线性模型只能挖掘特征之间的线性组合关系。

#### 核方法与支持向量机

**核方法**基本思想：通过一个非线性变换，把输入数据映射到高维的希尔伯特空间，在这个高维空间里，那些在原始输入空间中线性不可分的问题变得更容易解决，甚至线性可分。

SVM基本思想：通过核函数将原始输入空间变换成一个高维（甚至无穷维）空间，在这个空间里寻找一个超平面，使正负例之间**间隔最大化**。

常用核函数：

 - 多项式核：$(x_i^T x_j)^p, p\geq 1$
 - 高斯核：$exp(-\frac{||x_i-x_j||^2}{2\sigma^2})$
 - 拉普拉斯核：$exp(-\frac{||x_i-x_j||}{\sigma}), \sigma>0$
 - Sigmoid核：$tanh(\beta x_i^Tx_j+\theta), \beta>0, \theta<0$

任何一个核函数都隐式地定义了一个“再生核希尔伯特空间”（Reproducing Kernel Hilbert Space，RKHS），在这个空间里，两个向量的内积等于对应核函数的值。

**附**：
[拉格朗日乘数法几何意义](https://www.zhihu.com/question/38586401)

#### 决策树与Boosting

决策树的基本思想：根据数据的属性构造出树状结构的决策模型。叶子结点对应最终的决策结果，其他结点则针对数据的某种属性进行判断与分支。决策时可以实现比较复杂的非线性映射。

常用决策树算法有：分类及回归树（CART）、ID3、C4.5、决策树桩（Decision Stump）等。
两个基本步骤：

 1. 划分选择：根据某种准则（信息增益、增益率、基尼系数等）在某个结点上把数据集里的样本划分到它的一棵子树上。
 2. 剪枝处理：降低决策树复杂度，抑制过拟合。
	 - 预剪枝：在决策树生成过程中，对结点划分前进行估计，估计划分后能否带来**决策树泛化性能的提升**，若否则停止划分并且将当前结点标记为叶子结点。
	 - 后剪枝：先生成一棵完整的决策树，然后自底向上考察去掉每个结点后泛化能力是否有所提高，有则进行剪枝。

集成学习，常用方法有Bagging、Boosting等。
Boosting的基本思路：**每一轮训练加大关注错误样本**。
最终预测模型是所有若学习器的**加权求和**。Boosting在抵抗过拟合方面有非常好的表现。

**附**：
间隔定理（Margin Theory）：训练迭代增大分类置信度。
梯度提升决策树（GBDT）

#### 神经网络

**全连接神经网络**
通用逼近定理（Universal Approximation Theorem）

**CNN**
卷积就是**卷积核的各个参数**和**图像中空间位置对应的像素值**进行**点乘再求和**。
池化的目的是对原特征映射进行压缩，从而更好地体现图像识别的平移不变性，并且有效扩大后续卷积操作的感受野。

**RNN**
为解决RNN按时域展开后遇到的梯度消减问题，人们提出依靠门电路来控制信息流通的方法，且这个门电路也是带参数，参数在神经网络优化过程中可学习。
LSTM 和 GRU（GRU更简单且训练速度更快）：
![](https://cdn-images-1.medium.com/max/1600/0*1udenjz1XCZ5cHU4.)

RNN应用场景：

 - 图像配文字：点到序列
 - 情感分类：序列到点
 - 机器翻译：序列到序列。注意力机制

### 常用的优化方法

 - 确定性算法
 - 随机算法：解决大数据量下确定性算法的效率问题。基本思想是利用样本抽样信息进行计算，对目标函数进行优化。

**附**：
无偏估计
[机器学习非凸优化技术](https://www.jiqizhixin.com/articles/2017-12-29-4)

### 机器学习理论

#### 算法泛化误差

最小化期望损失风险 -> 最小化经验风险（正则化经验风险最小化）

优化算法对（正则化）经验风险最小化问题进行求解，希望学习到的模型的期望风险尽可能小，并将其定义为机器学习算法的泛化误差。

#### 泛化误差的分解

 - 优化误差：反映**算法**局限性。与优化算法、数据量大小、迭代轮数及函数空间有关。
 - 估计误差：反映**数据集**局限性。与数据量大小和函数空间复杂程度有关。
 - 近似误差：与**函数空间的表达力**有关。

定性描述：函数空间增大，近似误差减小、估计误差增大；数据量增大，估计误差减小；迭代轮数增大，优化误差减小。

#### 基于容度的估计误差的上界

一致估计偏差

容度：描述函数集合在多个输入数据上所产生的输出多样性。

 - VC维：描述函数集合最多能打散多少样本点。
 - 拉德马赫平均（Rademacher Average）
 - 覆盖数

## 分布式机器学习框架

### 数据与模型划分模块

 - 数据划分
	 - 对样本：随机采样；置乱切分
	 - 对特征维度：需与特定的优化方法配合使用
 - 模型划分：需考虑模型结构特点
	 - 横向划分
	 - 纵向划分
	 - 随机划分

### 单机优化模块

### 通信模块

#### 通信内容

 - 模型或模型的更新
 - 重要的样本

#### 通信拓扑结构

 - 迭代式MapReduce/AllReduce：抽象简单但不够灵活。运算结点和模型存储没有很好的逻辑隔离，只支持同步通信模式。如Spark MLlib
 - 基于参数服务器：把工作结点和模型存储结点逻辑上分开。如MXNet、Paddle、Petuum、KunPeng
 - 基于数据流：计算被描述成DAG，每个结点通常有两个通信通道：控制消息流和计算数据流。如TensorFlow

#### 通信步调

 - 同步：易受慢结点拖累
 - 异步：会引发“延迟”问题。解决方案有AdaptiveRevision和AdaDelay，或补偿延迟
 - 半同步（SSP）：设置延迟阈值，超过则快等慢
 - 混合同步：分组。组内同步、组间异步

#### 通信频率

 - 时域滤波：降低通信频率
 - 空域滤波：减少发送数据量。可采用模型压缩（如低秩分解）、模型量化（如降低精度）

### 数据与模型聚合模块

 - 模型平均或ADMM：操作简单，在学习目标是凸函数条件下不会损失精度。
 - 模型集成：可以保证模型聚合时精度不会损失。非凸问题适用

### 分布式机器学习理论

 - 收敛性
 - 加速比
 - 泛化性

### 分布式机器学习系统

 - 灵活性：PS > 迭代式MR > 数据流
 - 运行效率：PS, 数据量 > 迭代式MR
 - 处理的任务：
 - 用户使用：生态

## 单机优化之确定性算法

### 概述

#### 优化框架

 - 正则化经验风险最小化
 - 优化算法收敛速度：线性、次线性、超线性
 - 假设条件：凸性（强凸）；光滑性（Lipschitz连续）

#### 优化算法的分类和发展史

随机算法及它的并行版本对处理海量数据有很大优势。

### 一阶确定性算法

#### 梯度下降法

基本思想：最小化目标函数在当前状态的一阶泰勒展开，从而近似地优化目标函数本身。

更新规则：$w_{t+1}=w_t-\eta \nabla f(w_t)$，其中 $\eta >0$ 是步长，也称学习率。

针对不同性质的目标函数，梯度下降法具有不同的收敛速率

 - 强凸性质会大大提高梯度下降法的收敛速率
 - 光滑性质在凸和强凸两种情形下都会加快梯度下降法的收敛速率

局限

 - 只适用于无约束优化问题
 - 只适用于梯度存在的目标函数

#### 投影次梯度下降法

先进行梯度下降再对约束进行投影。适用于**有约束优化**问题。

#### 近端梯度下降法

是投影次梯度下降法的推广，适用于不可微的凸目标函数的优化问题。
在一定条件下可以取得线性收敛速率。

#### Frank-Wolfe算法

在最小化目标函数的泰勒展开时就将约束条件考虑进去。
是投影次梯度下降法的推广另一个替代算法，也可用于有约束优化问题，效率更高。

#### Nesterov加速法

TODO.

#### 坐标下降法

思想：在迭代的每一步，算法选择一个维度（或进行分块），并更新这一维度（或块中的维度），其他维度参数保持不变。

### 二阶确定性算法

#### 牛顿法

思想：二阶泰勒展开

提供了更为精细的步长调节，收敛速率比梯度下降法收敛速率显著加快，具有二次收敛速率。

问题

 - 计算量和存储量显著加大
 - 海森矩阵不一定正定

#### 拟牛顿法

思想：构造一个与海森矩阵相差不太远的正定矩阵作为其替代品。

### 对偶方法

把原始优化问题转化为对偶优化问题。

### 总结

确定性算法是优化的基石，随机优化算法就是建立在确定性算法纸上的。

## 单机优化之随机算法

### 基本随机优化算法

通过对样本和维度进行随机采样，来得到对更新量的有效估计或者替代。

#### 随机梯度下降法（SGD）

经验损失函数定义为所有样本数据对应的损失函数的平均值，用有放回随机采样获得的数据来计算梯度，是对用全部数据来计算梯度的一个无偏估计。
虽然随机梯度是全梯度的无偏估计，但这种估计存在一定方差，会导致算法收敛速率下降。小批量可以有效减小方差，提高收敛速率。

#### 随机坐标下降法

对模型维度进行采样。

“可分离”情形：偏导数的计算量小于梯度的计算量。

小批量版本：随机块坐标下降法

#### 随机拟牛顿法

#### 随机对偶坐标上升法

#### 收敛性比较

 - 当数据量较大时，随机梯度下降法比梯度下降法更高效
 - 若目标函数可分离，随机坐标下降法比梯度下降法更高效
 - 若目标函数可分离，且数据维度较高，随机坐标下降法比随机梯度下降法更高效

### 随机优化算法的改进

#### 方差缩减

 - 对随机梯度加入正则项：SVRG、SAG、SAGA算法。收敛率可以从次线性提高到线性
 - 改进的采样方法：小批量采样；带权重采样（如基于重要性采样）

#### 算法组合

 - 随机方差缩减梯度法：收敛速率由次线性加速到线性
 - Nesterov加速法：使收敛速率获得多项式阶的加速
 - 随机坐标下降法：在保持收敛率的情况下使每一轮计算梯度复杂度下降

### 非凸随机优化算法

#### Ada系列算法

更新模型时，不只利用当前梯度，还利用历史上所有的梯度信息，并且自适应地（Adaptively）调整步长。

 - Momentum SGD
 - AdaGrad
 - RMSProp
 - AdaDelta
 - Adam：依照**梯度平方累加值**调整步长，依照**梯度累加值**更新模型。效果最好，在神经网络训练中应用最广
	 - 更新方向由历史梯度累计决定
	 - 对步长采用累加的梯度平方值进行修正
	 - 信息累加按照指数形式衰减

#### 非凸理论分析

#### 逃离鞍点

思想：如果局部极小值点具有相对**良好**的性质，引导优化过程逃出鞍点。

#### 等级优化算法（Graduated Optimization）

思想：减少陷入**不良**的局部极小值点。

步骤：

 1. 通过局部磨光算子将目标函数转变为一个光滑函数
 2. 用随机优化算法最小化这个光滑函数
 3. 将算法的解作为下一轮优化的起点，减小磨光粒度（逐步逼近原来的目标函数），返回步骤1

## 数据与模型并行

### 计算并行模式

并行随机梯度下降法收敛性的衡量准则

 - 数据在线生成：后悔度（regret）
 - 数据离线生成：次优性准则

### 数据并行模式

#### 数据样本划分

样本划分的合理性：因为经验风险函数是所有训练样本对应的损失函数取值的加和，如果将训练样本划分为多个数据子集，计算各个子集上的局部梯度值，再将局部梯度加和，仍然可以得到整个经验风险函数的梯度。

划分方法：

 - 随机采样：有放回
 - 置乱切分：样本被随机置乱成K份均分到K个工作结点。相当于无放回。实际中使用更多。
好处：复杂度较低；数据信息量比较大
数据使用一遍后的再处理：全局置乱切分；局部置乱切分

#### 数据维度划分

一般情况下，损失函数关于维度并不是可分的。
维度划分适用的机器学习任务有：

 - 决策树方法
 - 线性模型：模型参数与数据维度一一对应

### 模型并行模式

#### 线性模型

可分性模型进行模型并行，对模型和数据按维度均等划分分配到不同结点，再利用坐标下降法进行优化。本地参数更新只需对全局变量进行通信，不需要对其他结点模型参数进行通信。

#### 神经网络

 - 横向按层划分：通常会结合各层结点数目，尽可能使得各个工作结点的计算量平衡。适用每层神经元数目不多而层数很多。
 - 纵向跨层划分：适用每层神经元数目很多而层数较少。
 - **模型随机并行**：按某种准则（如连边的重要性或神经元的重要性）在原网络中选出**骨干网络**（神经网络的冗余性使得存在一个规模小很多而效果与原网络差不多的骨干网络），作为公共子网络存储于每个工作结点。此外每个结点还会随机选取一些其他结点存储，以探索骨架之外的信息。骨架网络周期性地依据新的网络重新选取，用于探索的结点也会每次随机选取。

## 通信机制

分布式机器学习的关键是设计通信机制（主要利用机器学习数值优化算法的容错性特点），降低通信与计算的时间比。

### 通信的内容

 - 参数或参数的更新：数据并行下。
 - 计算的中间结果：模型并行下。
 - 重要样本：基于数据交互的数据并行模式
 - 样本的预测值：通常与集成学习配合

通信的目的也可以不断突出各个工作结点在某方面的判断能力，不必完全一致，保持各个工作结点上的模型在各自擅长处理的部分样本上达到较高的精度，使得在**整个样本空间**更加出色的完成预测任务。

### 通信的拓扑结构

 - 基于迭代式MapReduce/AllReduce：只支持同步通信；要求每个工作结点能够处理完整模型
采用IMR时由系统本身决定；
采用MPI的AllReduce接口时，基于ReduceScatter和AllGather的实现在传输量和传输次数上都有一定优势。
 - 基于参数服务器：逻辑上采用二部图通信拓扑结构。工作结点和服务器结点间通信，工作结点间无需通信
 - 基于数据流：每个结点有两个通信通道，控制消息流和计算数据流
	 - 控制消息流：决定工作结点应该接收什么数据，接收数据是否已完整，本地计算是否完成，是否可以让下游结点继续计算等。
	 - 计算数据流：接收模型训练所需数据、模型参数等，并将计算产生的输出数据提供给下游结点。

### 通信的步调

#### 同步通信

同步屏障（Barrier Synchronization）

#### 异步通信

 - 多机异步通信：“延迟”问题
 - 多线程异步通信：Hogwild! 算法：每个线程无锁读取和写入模型及其更新（在优化目标为凸函数且模型更新比较稀疏的情况下，异步无锁的写入不会对收敛性造成本质影响）

实践中通常会结合单机共享内存的本地加速方法&多机同步或异步的分布式机制共同完成大规模的机器学习任务。

#### 同步和异步的平衡

场景：实际中往往采用相对同质化的集群，机器快慢大都是随机的。
**延迟同步并行（Stale Synchronous Parallel，SSP）**：控制最快和最慢结点之间相差的迭代次数不超过预设的阈值。
[Adam](https://web.eecs.umich.edu/~mosharaf/Readings/Project-Adam.pdf)系统方案：维持一个全局时钟，发现更新太陈旧的结点时，丢弃并刷新为餐宿服务器上的最新模型。

### 通信的频率

#### 时域滤波

 - 增加通信间隔：本地模型多次更新后才通信一次。可用于每层都是全连接层的深层神经网络
 - 非对称的推送和获取：对推送模型更新和获取最新的模型两种操作采用不同的频率
 - **计算和传输流水线**：利用机器学习的容错性，将两次迭代之间的计算和通信用流水线方式并行起来。

#### 空域滤波

 - 模型过滤：过滤掉没有明显变化的参数。
 - 模型**低秩化处理**：通过矩阵低秩分解（如SVD）将原来比较大的参数矩阵分解成几个比较小的矩阵的乘积，传输之后再重新恢复成较大的参数矩阵。
 - 模型量化：降低参数浮点数的精度。**一比特量化方法**每次量化时记录量化误差，并将其与 梯度一起计算在下次要量化的对象里。

## 数据与模型聚合

聚合也是分布式机器学习特有的逻辑。

### 基于模型加和

主要用于数据并行。

#### 基于全部模型加和

 - 模型平均（MA）：对**参数**进行平均得到新的全局模型
 - BMUF：在MA基础上加入冲量
 - ADMM：全局一致性优化
 - 同步随机梯度下降法（SSGD）：对**梯度**进行平均
 - 弹性平均随机梯度下降法（EASGD）：引入弹性机制，将工作节点的模型平均值与服务器端参数线性加和

#### 基于部分模型加和

 - 带备份结点的同步随机梯度下降法：空间换时间，用$K(1+\alpha)$个工作节点保证每次取前$K$个作为聚合对象
 - 异步ADMM：设置一个最少同步工作结点数；同时控制最大延迟。训练速度明显快于标准ADMM方法。适合在参数服务器框架下实现。
异步ADMM算法允许部分更新较慢结点暂时不参与当前全局变量更新，当它的更新最终到达参数服务器时，可以和其他结点的更新一起参与下一轮的全局变量更新。
 - 去中心化方法：网络通信代价小，效率高。同时还能在各个工作结点维护自己的模型，提高整个系统的鲁棒性。如D-PSGD算法

### 基于模型集成

模型集成：对模型输出进行加和或平均的方法。

#### 基于输出加和

“集成-压缩”算法。引入了模型压缩环节。对非凸优化问题和网络环境比较差的场景行之有效。

**知识蒸馏**：首先使用大规模的集成模型对样本进行再标注，然后指定一个小规模的模型，按照样本的输入特征和新的标注信息训练小规模模型中的参数（将大模型所包含的知识在样本标签相同的意义下蒸馏到小模型中）。

#### 基于投票的聚合

两级投票策略：首先本地**预计算**选出前a个特征告知中心服务器，中心服务器选出**全局**前a个最佳特征，通信要求工作结点把这a个特征对应的直方图信息汇总到中心服务器。然后中心服务器根据这些直方图信息**再进行一次精准计算**，选出一个全局最佳特征，并计算它的最佳分割点。

基于投票的聚合方法相较于决策树经典并行方法能够以更快速度收敛，且模型精度几乎没有损失。

## 分布式机器学习算法

不同的数据与模型划分、单机优化方法、通信机制、数据与模型聚合方式的组合，会产生不同的算法。

### 同步算法

 - 同步SGD
 - 模型平均方法及其改进
 - ADMM算法
 - 弹性平均SGD算法

### 异步算法

 - 异步SGD
 - Hogwild! 算法
 - Cyclades算法
 - 带延迟处理的异步算法
	 - AdaDelay算法
	 - 带有延迟补偿的ASGD算法 DC-ASGD

### 同步和异步的对比与融合

#### 对比

#### 融合

**混合并行**算法：对工作结点按照某种指标进行聚类分组，组内同步并行、组间异步并行。适合**异构**集群。

可用元学习（meta learning 或 learning to learn）的思路寻找合理的分组方式：首先针对工作结点和运行的学习任务提取一系列特征。**利用工作结点的特征**，采用层次聚类的方法得到若干候选分组。然后用这些候选分组在抽样的数据集上进行试训练，得到分布式训练的速度。而后**针对这些分组和学习任务的特征**，训练一个预测优化速度的神经网络模型（如RNN）。之后利用这个模型对于未知的候选分组进行打分，最终找到这个任务的最好分组。

### 模型并行算法

 - DistBelief
 - AlexNet

## 分布式机器学习理论

以泛化能力为最终目标。

### 收敛性分析

#### 优化目标和算法

#### 数据和模型并行

#### 同步和异步

### 加速比分析

#### 从收敛速率到加速比

#### 通信量的下界

算法收敛到最优模型需要满足的最小通信量。

### 泛化分析

#### 优化的局限性

#### 具有更好泛化能力的非凸优化算法

## 分布式机器学习系统

通用的分布式机器学习架构：隔离系统和学习

 - 实现并优化了通用的系统问题，如调度、局部性、容灾、网络传输等；
 - 抽象出分布式机器学习的核心算法逻辑，并提供高层次API

### 基于IMR

适用场景：同步 + 数据并行

很难实现对局部参数的访问和对参数历史状态的访问。

#### Spark

通过窄依赖（同一分区中计算的相互依赖）尽量减少无必要的通信，只在必要时触发宽依赖（不同分区间...）。

每个DAG实际上由一系列互相依赖的RDD组成。RDD可以跨越整个DAG存在，可以把一些要重复使用的数据存在全局的RDD中。

RDD提供两类操作：

 - 转换操作：定义新的RDD
 - 动作操作：返回对现有RDD的操作和改变

### 基于参数服务器

同时支持同步和异步的并行算法。

参数进行分布式存储的考虑：

 - 更快的响应
 - 可扩展性
 - 系统容错

### 基于数据流

#### TensorFlow

系统收到用户定义的数据流后，首先对该数据流图进行优化，包括裁剪移除一些无用操作、内存优化等。然后根据分配算法，将图中计算结点分配到实际的运算设备上分布式地执行这些计算任务。

训练过程中，Tensor在数据流图中流动。

TensorFlow两种数据对象：

 - 张量：如结点的输出。张量作为中间值用过以后就释放了
 - 变量：如模型参数

角色：Client、Master、Worker

为支持分布式机器学习，TensorFlow使用Cluster、Job和Task来定义一个计算集群。

两种分布式训练模式

 - In-Graph：只有一个Client。主要用于单机多卡训练场景
 - Between-Graph：有多个Client，既可用于单机多卡，也可用于多机多卡的分布式训练场景。

## 结语

### 未来展望

自适应、自演化
